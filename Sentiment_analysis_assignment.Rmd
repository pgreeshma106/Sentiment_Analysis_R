---
output:
  word_document: default
  html_document: default
---

# Greeshma Chowdary Peddineni

```{r}
mcu_ethics <- readRDS("C:/Users/Dell/Downloads/mcu_ethics.rds")
```

```{r}
library(tidytext)
library(ggplot2)
library(dplyr)
library(tidyr)
library(textdata)
library(wordcloud)
library(reshape2) 
library(tm) 
library(widyr)
library(igraph)
library(ggraph)
```

```{r}
get_sentiments("nrc")
get_sentiments("bing")
get_sentiments("afinn")

```


```{r}
# Let's find how many English tweets are there? 
mcuEN <- mcu_ethics %>% filter(lang=="en") 
print(paste("English tweets are", count(mcuEN)))
```


```{r}
# What is the ratio of retweets to total tweets?
retweets <- mcu_ethics %>% filter(grepl("^RT", msg)) %>% nrow()
total_tweets <- nrow(mcu_ethics)
retweet_ratio <- retweets / total_tweets
print(paste("Ratio of retweets to total tweets:", retweet_ratio))
```

```{r}
# How many tweets contain a hyperlink?
hyperlink_tweets <- mcu_ethics %>% filter(grepl("http[s]?://", source)) %>% nrow()
print(paste("Number of tweets containing a hyperlink:", hyperlink_tweets))
```

```{r}
# what is the most popular “source” used to post? 

most_popular_source <- mcu_ethics %>% 
  count(source, sort = TRUE) %>% 
  slice_max(n, n = 1)

cat("Most popular source:", most_popular_source$source, "\n")
cat("Count:", most_popular_source$n, "\n")

```


```{r}
# Create a bar graph of most frequent Positive and Negative words (using BING)
# Load the Bing sentiment lexicon
bing_lexicon <- get_sentiments("bing")

tweet_words <- mcu_ethics %>% 
  unnest_tokens(word, msg) %>% 
  filter(word != "marvel") %>%  # Remove the word "marvel, because we cannot say it is a positive or negative word."
  inner_join(bing_lexicon, by = "word") 

# Get top 10 positive and negative words separately
top_positive <- tweet_words %>%
  filter(sentiment == "positive") %>%
  count(word, sentiment, sort = TRUE) %>%
  top_n(10, n)

top_negative <- tweet_words %>%
  filter(sentiment == "negative") %>%
  count(word, sentiment, sort = TRUE) %>%
  top_n(10, n)

# Combine both into one dataframe
top_words <- bind_rows(top_positive, top_negative)

# Ensure sentiment is a factor to avoid issues in facet_wrap
top_words$sentiment <- factor(top_words$sentiment, levels = c("negative", "positive"))

# Create bar plot
ggplot(top_words, aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  labs(title = "Most Frequent Positive and Negative Words", x = "Words", y = "Frequency") +
  theme_minimal()
```


```{r}
# Let's create a wordcloud to get most frequent positive and negative words
library(tm)
library(textdata)

tweet_words %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "blue"), scale = c(4, 1),
                   random.order = FALSE, max.words = 60)
```

```{r}
# Let's do create a network diagram (adjust the filter so that individual words are legible. ie: don't have a tangled mess of a plot) 
tweet_bigrams <- mcu_ethics %>%
  unnest_tokens(bigram, msg, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(!is.na(word1) & !is.na(word2)) %>%
  count(word1, word2, sort = TRUE)

# Filter for frequent bigrams 
sotuBigramCount <- tweet_bigrams %>% filter(n >= 2000)

# Convert to graph object
graph <- graph_from_data_frame(sotuBigramCount)

# Create network plot
ggraph(graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), show.legend = FALSE) +
  geom_node_point(color = "darkslategray4", size = 3) +
  geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
  labs(title = "Word Network: Tweets",
       subtitle = "Text Mining of Tweet Bigrams",
       x = "", y = "") +
  theme_void()

```









